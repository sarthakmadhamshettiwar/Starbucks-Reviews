# -*- coding: utf-8 -*-
"""Starbucks Review Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Lv4uTppHb3CDVEqA-EXgJUimfL6ndPZ

### Installing Libraries and gathering Data
"""

!pip install emoji
!pip install xgboost

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno
import emoji
import spacy
nlp = spacy.load('en_core_web_sm')

import nltk
import string
from nltk.tokenize import word_tokenize
import re
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
import xgboost as xgb
from xgboost import XGBClassifier


from tensorflow.keras.models import Sequential
from tensorflow.keras import regularizers
from tensorflow.keras.layers import GlobalMaxPooling1D,Embedding,Dense, LSTM

from sklearn.metrics import accuracy_score

nltk.download('punkt')
nltk.download('wordnet')

data = pd.read_csv('reviews_data.csv')

data.head()

data.isnull().sum()

data.columns

data.shape

# We can't use this column as most of the reviews doesn't have images associated with them.
data['Image_Links'].value_counts()

data['location'].value_counts()

data.dropna(inplace = True)
data.drop(columns = ['Image_Links'], inplace = True)

"""### EDA

**Ratings**
"""

sns.countplot(x = "Rating", data = data)

"""1.   Imbalanced Dataset, with mostly negative ratings
2.   Overall ratings no longer matter, instead Positive, Negative, and Neutral classes will be more benificial for Company's ananlysis.


"""

data["Rating"]=data["Rating"].replace([1,2],0)
data["Rating"]=data["Rating"].replace(3,1)
data["Rating"]=data["Rating"].replace([4,5],2)
data = data.dropna(axis = 0, how ='any')
data["Rating"]=data["Rating"].astype(int)

data["Rating"].value_counts()

plt.figure(figsize=(6, 6))
sns.set_style("whitegrid")
plt.pie(data["Rating"].value_counts(),labels=["Negative","Positive","Neural"], autopct='%1.1f%%', startangle=90);

"""**Location**"""

# Instead of City, we will only use State or more general address for locating a Starbucks Restaurant
data["location"]=data["location"].str.split(",").str[1]

data['location'].unique()

plt.figure(figsize=(10,8))
sns.set_style("whitegrid")
data["location"].value_counts().sort_values(ascending= False).head(10).plot.bar()
plt.title("Count Of Reviews per State");

"""*   Most of the reviews come from CA, but till now we have no way to find that whether a review was positive, negative or neutral.
*   We should try to analyse relation between location and reviews if any.
*   We will pick 9 most popular outlets of Starbucks.

**Relation between Location and Ratings**
"""

df = data[["location","Rating"]]
df["Negative"] = df[df["Rating"]==0]["Rating"]
df["Positive"] = df[df["Rating"]==2]["Rating"]
df["Neutral"] = df[df["Rating"]==1]["Rating"]
df.drop("Rating",axis=1,inplace=True)
df=df.groupby("location").count().sort_values(ascending= False,by='Negative').head(9)

fig = plt.figure(figsize = (25, 25))
i=1
features = df.index
for x in features:
    plt.subplot(3, 3, i)
    ax=plt.pie(df.loc[x], labels=df.columns, autopct='%1.1f%%', startangle=90,
        colors=['#FF5959', '#676FA3','#CDDEFF'])
    plt.title(str(x), loc='center')
    i+=1
plt.show()

"""Almost all the popular outlets of Starbucks have negative reviews, some of them have some percent of neutral and positive reviews but they are very less in numbers compared to negative reviews

**Day and Month wise Reviews**
"""

data['Date'] = data['Date'].str.replace("Reviewed", "", case=False, regex=False)
data = data.astype({'Date': 'datetime64[ns]'})
data['Month'] = data['Date'].dt.month
data['Year'] = data['Date'].dt.year
data['DayOfWeek'] = data['Date'].dt.dayofweek

day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
df = data.groupby(['DayOfWeek'])['Rating'].sum().reset_index()

plt.figure(figsize=(10, 6))
plt.bar(df['DayOfWeek'], df['Rating'], color='skyblue')
plt.ylabel('Total Rating')
plt.title('Total Rating Count Per Day of the Week')
plt.xticks(df['DayOfWeek'], labels=day_order);

"""

*   Almost same number of cusotmers are visiting on weekends, and relatively more cusomers are visiting in the middle of the weeks.
*   Weekend Revies counts are very less. Company can take efforts to increase attract the customers on Weekends which may include giving discount offers and etc.

"""

df = data.groupby(['Month'])['Rating'].sum().reset_index()
month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

plt.figure(figsize=(10, 6))
plt.plot(df['Month'], df['Rating'], marker='o', color='skyblue', linestyle='-')
plt.xlabel('Month')
plt.ylabel('Total Rating')
plt.title('Total Rating Count Per Month')

plt.xticks(df['Month'], labels=month_order,rotation=45)
plt.grid(True)
plt.show()

"""### Cleaning and Preprocessing Data"""

data = data[["Review","Rating"]]

def cleaning (text):
    text = re.sub('[^a-zA-Z]', ' ', str(text).lower().strip())
    text = re.sub('@[A-Za-z0-9_]+', '', text)
    text = re.sub('#','',text)
    text = re.sub('RT[\s]+','',text)
    text = re.sub('https?:\/\/\S+', '', text)
    text = re.sub('\n',' ',text)
    text = emoji.replace_emoji(text, replace='')
    return text

data["Review"]=data["Review"].apply(cleaning)

def lemm(data):
    wordnet = WordNetLemmatizer()
    lemmanized = []
    for i in range(len(data)):
        lemmed = []
        words = word_tokenize(data['Review'].iloc[i])
        for w in words:
            lemmed.append(wordnet.lemmatize(w))
        lemmanized.append(lemmed)

    data['lemmanized'] = lemmanized
    data['Review'] = data['lemmanized'].apply(' '.join)
    data=data.drop("lemmanized",axis=1)
    return data
data=lemm(data)

data.head()

labels = data["Rating"]
training = data["Review"]

X_train, X_test, y_train, y_test = train_test_split(training, labels, test_size=0.2, random_state=42)

"""**Feature Extraction**"""

# We can remove stopwords to improve accuracy and reduce meaningless words.
stop_words = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]

vectorizer = TfidfVectorizer(stop_words = stop_words)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

"""### Training Models

**Logistic Regression**
"""

model= LogisticRegression(multi_class="multinomial")
model.fit(X_train_vec,y_train)
preds =model.predict(X_test_vec)
print(accuracy_score(y_test,preds))

"""**Random Forest**"""

model = RandomForestClassifier()
model.fit(X_train_vec,y_train)
preds = model.predict(X_test_vec)
print(accuracy_score(y_test,preds))

"""**SVM**"""

model = LinearSVC()
model.fit(X_train_vec,y_train)
preds = model.predict(X_test_vec)
print(accuracy_score(y_test,preds))

"""**Naive Bayes**"""

model = MultinomialNB()
model.fit(X_train_vec,y_train)
preds = model.predict(X_test_vec)
print(accuracy_score(y_test,preds))

"""**XGBoost Classifier**"""

model = XGBClassifier(
    objective='multi:softmax',
    num_class=3
)
model.fit(X_train_vec, y_train)
preds = model.predict(X_test_vec)
print(accuracy_score(y_test, preds))

